#!/usr/bin/env python3
"""
Simplified Terragon Value Discovery Engine
For environments without full Python toolchain
"""

import json
import subprocess
import re
import os
from datetime import datetime
from pathlib import Path


def discover_git_items():
    """Discover TODO/FIXME items from code."""
    items = []
    try:
        # Find TODO/FIXME/HACK comments
        result = subprocess.run([
            'grep', '-r', '-n', '-i', 
            '--include=*.py', '--include=*.yaml', '--include=*.md',
            r'\(TODO\|FIXME\|HACK\|XXX\)',
            '.'
        ], capture_output=True, text=True)
        
        for line in result.stdout.split('\n'):
            if line.strip():
                match = re.match(r'([^:]+):(\d+):(.+)', line)
                if match:
                    file_path, line_num, content = match.groups()
                    
                    category = "technical-debt"
                    priority = 30
                    
                    if "TODO" in content.upper():
                        category = "feature-enhancement"
                        priority = 25
                    elif "FIXME" in content.upper():
                        category = "bug-fix"
                        priority = 40
                    elif "SECURITY" in content.upper():
                        category = "security-fix"
                        priority = 60
                    elif "PERFORMANCE" in content.upper():
                        category = "performance-optimization"
                        priority = 35
                    
                    items.append({
                        'id': f"git-{hash(line) & 0x7fffffff}",
                        'title': f"Address {category.replace('-', ' ')} in {file_path}",
                        'description': content.strip(),
                        'category': category,
                        'priority': priority,
                        'effort_hours': 2,
                        'file_path': file_path,
                        'line_number': line_num
                    })
    except subprocess.CalledProcessError:
        pass
    
    return items


def discover_security_items():
    """Discover security vulnerabilities."""
    items = []
    
    # Check for hardcoded secrets patterns
    secret_patterns = [
        r'password\s*=\s*["\'][^"\']+["\']',
        r'api_key\s*=\s*["\'][^"\']+["\']',
        r'secret\s*=\s*["\'][^"\']+["\']',
        r'token\s*=\s*["\'][^"\']+["\']'
    ]
    
    for pattern in secret_patterns:
        try:
            result = subprocess.run([
                'grep', '-r', '-n', '-i', '--include=*.py', pattern, '.'
            ], capture_output=True, text=True)
            
            for line in result.stdout.split('\n'):
                if line.strip() and 'example' not in line.lower():
                    match = re.match(r'([^:]+):(\d+):(.+)', line)
                    if match:
                        file_path, line_num, content = match.groups()
                        items.append({
                            'id': f"sec-{hash(line) & 0x7fffffff}",
                            'title': f"Potential hardcoded secret in {file_path}",
                            'description': f"Possible hardcoded credential detected: {content.strip()[:50]}...",
                            'category': 'security-fix',
                            'priority': 80,
                            'effort_hours': 1,
                            'file_path': file_path,
                            'line_number': line_num
                        })
        except subprocess.CalledProcessError:
            pass
    
    return items


def discover_performance_items():
    """Discover performance optimization opportunities."""
    items = []
    
    # Look for potential performance issues
    perf_patterns = [
        (r'\.append\(.*\)\s*for.*in', 'List comprehension opportunity'),
        (r'for.*in.*:\s*if.*:', 'Filter opportunity'),
        (r'time\.sleep\(', 'Synchronous sleep detected')
    ]
    
    for pattern, description in perf_patterns:
        try:
            result = subprocess.run([
                'grep', '-r', '-n', '--include=*.py', pattern, 'src/'
            ], capture_output=True, text=True)
            
            for line in result.stdout.split('\n'):
                if line.strip():
                    match = re.match(r'([^:]+):(\d+):(.+)', line)
                    if match:
                        file_path, line_num, content = match.groups()
                        items.append({
                            'id': f"perf-{hash(line) & 0x7fffffff}",
                            'title': f"Performance optimization in {file_path}",
                            'description': f"{description}: {content.strip()[:50]}...",
                            'category': 'performance-optimization',
                            'priority': 35,
                            'effort_hours': 3,
                            'file_path': file_path,
                            'line_number': line_num
                        })
        except subprocess.CalledProcessError:
            pass
    
    return items


def discover_documentation_items():
    """Discover documentation improvements."""
    items = []
    
    # Check for Python files without docstrings
    try:
        python_files = subprocess.run([
            'find', 'src/', '-name', '*.py', '-type', 'f'
        ], capture_output=True, text=True).stdout.strip().split('\n')
        
        for file_path in python_files:
            if file_path.strip():
                try:
                    with open(file_path, 'r') as f:
                        content = f.read()
                        
                    # Check for functions without docstrings
                    func_matches = re.findall(r'def\s+(\w+)\s*\([^)]*\):', content)
                    docstring_matches = re.findall(r'""".*?"""', content, re.DOTALL)
                    
                    if len(func_matches) > len(docstring_matches):
                        items.append({
                            'id': f"doc-{hash(file_path) & 0x7fffffff}",
                            'title': f"Add docstrings to {file_path}",
                            'description': f"Found {len(func_matches)} functions but only {len(docstring_matches)} docstrings",
                            'category': 'documentation',
                            'priority': 20,
                            'effort_hours': 2,
                            'file_path': file_path
                        })
                except:
                    pass
    except subprocess.CalledProcessError:
        pass
    
    return items


def calculate_composite_score(item):
    """Calculate composite value score."""
    # Base score from priority
    base_score = item['priority']
    
    # Adjust based on effort (prefer low effort, high impact)
    effort_factor = max(0.5, 5 / item['effort_hours'])
    
    # Category multipliers
    category_multipliers = {
        'security-fix': 2.0,
        'bug-fix': 1.5,
        'performance-optimization': 1.3,
        'feature-enhancement': 1.2,
        'technical-debt': 1.1,
        'documentation': 0.8
    }
    
    multiplier = category_multipliers.get(item['category'], 1.0)
    
    return base_score * effort_factor * multiplier


def generate_backlog_markdown(items):
    """Generate markdown backlog report."""
    if not items:
        return "# ğŸ“Š Autonomous Value Backlog\n\nNo items discovered."
    
    # Sort by composite score
    for item in items:
        item['composite_score'] = calculate_composite_score(item)
    
    sorted_items = sorted(items, key=lambda x: x['composite_score'], reverse=True)
    next_item = sorted_items[0] if sorted_items else None
    
    report = f"""# ğŸ“Š Autonomous Value Backlog

Last Updated: {datetime.now().isoformat()}
Total Items Discovered: {len(items)}
Repository: iot-edge-graph-anomaly
Maturity Level: ADVANCED (92%)

## ğŸ¯ Next Best Value Item
"""
    
    if next_item:
        report += f"""**[{next_item['id'].upper()}] {next_item['title']}**
- **Composite Score**: {next_item['composite_score']:.1f}
- **Category**: {next_item['category'].replace('-', ' ').title()}
- **Priority**: {next_item['priority']}/100
- **Estimated Effort**: {next_item['effort_hours']} hours
- **File**: {next_item.get('file_path', 'N/A')}

{next_item['description']}

"""
    
    report += """## ğŸ“‹ Top 10 Backlog Items

| Rank | ID | Title | Score | Category | Hours | Priority |
|------|-----|--------|---------|----------|-------|----------|
"""
    
    for i, item in enumerate(sorted_items[:10], 1):
        title = item['title'][:45] + ('...' if len(item['title']) > 45 else '')
        category = item['category'].replace('-', ' ').title()
        report += f"| {i} | {item['id']} | {title} | {item['composite_score']:.1f} | {category} | {item['effort_hours']} | {item['priority']} |\n"
    
    # Category breakdown
    categories = {}
    for item in items:
        cat = item['category'].replace('-', ' ').title()
        categories[cat] = categories.get(cat, 0) + 1
    
    report += f"""
## ğŸ“ˆ Discovery Statistics

### Items by Category
"""
    for category, count in sorted(categories.items()):
        report += f"- **{category}**: {count} items\n"
    
    report += f"""
### Value Distribution
- **High Priority** (>50): {len([i for i in items if i['priority'] > 50])} items
- **Medium Priority** (25-50): {len([i for i in items if 25 <= i['priority'] <= 50])} items  
- **Low Priority** (<25): {len([i for i in items if i['priority'] < 25])} items

### Effort Distribution
- **Quick Wins** (1-2 hours): {len([i for i in items if i['effort_hours'] <= 2])} items
- **Medium Tasks** (3-5 hours): {len([i for i in items if 3 <= i['effort_hours'] <= 5])} items
- **Large Tasks** (>5 hours): {len([i for i in items if i['effort_hours'] > 5])} items

## ğŸ”„ Autonomous Execution Status

- **Value Discovery Engine**: Active
- **Next Execution**: Continuous (on PR merge)
- **Learning Mode**: Enabled
- **Repository Maturity**: ADVANCED
- **Autonomous Mode**: Ready for execution

### Recommended Action Plan

1. **Immediate** (Next 24h): Execute highest-scoring security and bug fixes
2. **Short-term** (1 week): Address performance optimizations and technical debt
3. **Medium-term** (1 month): Complete feature enhancements and documentation

---
*Generated by Terragon Autonomous SDLC Engine v1.0*
*Repository Assessment: ADVANCED (92% maturity)*
"""
    
    return report


def save_metrics(items):
    """Save discovery metrics."""
    metrics = {
        "timestamp": datetime.now().isoformat(),
        "repository": "iot-edge-graph-anomaly",
        "maturity_level": "ADVANCED",
        "maturity_score": 92,
        "discovery_stats": {
            "total_items": len(items),
            "average_priority": sum(item['priority'] for item in items) / len(items) if items else 0,
            "average_effort": sum(item['effort_hours'] for item in items) / len(items) if items else 0,
            "categories": {}
        },
        "value_metrics": {
            "high_priority_items": len([i for i in items if i['priority'] > 50]),
            "quick_wins": len([i for i in items if i['effort_hours'] <= 2 and i['priority'] > 30]),
            "security_items": len([i for i in items if i['category'] == 'security-fix']),
            "total_estimated_hours": sum(item['effort_hours'] for item in items)
        }
    }
    
    # Count categories
    for item in items:
        cat = item['category']
        metrics["discovery_stats"]["categories"][cat] = metrics["discovery_stats"]["categories"].get(cat, 0) + 1
    
    # Save metrics
    os.makedirs('.terragon', exist_ok=True)
    with open('.terragon/value-metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)


def main():
    """Main value discovery execution."""
    print("ğŸ” Starting Terragon Autonomous Value Discovery...")
    
    # Discover from all sources
    all_items = []
    
    print("  ğŸ“ Discovering from Git history...")
    all_items.extend(discover_git_items())
    
    print("  ğŸ”’ Discovering security items...")
    all_items.extend(discover_security_items())
    
    print("  âš¡ Discovering performance opportunities...")
    all_items.extend(discover_performance_items())
    
    print("  ğŸ“š Discovering documentation gaps...")
    all_items.extend(discover_documentation_items())
    
    print(f"ğŸ“Š Total items discovered: {len(all_items)}")
    
    # Generate backlog report
    report = generate_backlog_markdown(all_items)
    with open('BACKLOG.md', 'w') as f:
        f.write(report)
    print("ğŸ“ Updated BACKLOG.md")
    
    # Save metrics
    save_metrics(all_items)
    print("ğŸ’¾ Saved metrics to .terragon/value-metrics.json")
    
    # Show next best item
    if all_items:
        for item in all_items:
            item['composite_score'] = calculate_composite_score(item)
        
        best_item = max(all_items, key=lambda x: x['composite_score'])
        print(f"ğŸ¯ Next best value: [{best_item['id']}] {best_item['title']}")
        print(f"   Score: {best_item['composite_score']:.1f} | Category: {best_item['category']} | Effort: {best_item['effort_hours']}h")
    else:
        print("âœ… No actionable items found - repository is in excellent state!")


if __name__ == "__main__":
    main()